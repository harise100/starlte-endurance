From stable-bounces@linux.kernel.org Wed Jul 23 02:21:59 2008
From: Marcelo Tosatti <mtosatti@redhat.com>
Date: Wed, 23 Jul 2008 11:43:39 +0300
Subject: KVM: mmu_shrink: kvm_mmu_zap_page requires slots_lock to be held
To: stable@kernel.org
Cc: Marcelo Tosatti <mtosatti@redhat.com>
Message-ID: <1216802623-31090-4-git-send-email-avi@qumranet.com>


From: Marcelo Tosatti <mtosatti@redhat.com>

Original-Commit-Hash: 64f6a0c041bd8fc100a0d655058bdbc31feda03c

kvm_mmu_zap_page() needs slots lock held (rmap_remove->gfn_to_memslot,
for example).

Since kvm_lock spinlock is held in mmu_shrink(), do a non-blocking
down_read_trylock().

Untested.

Signed-off-by: Avi Kivity <avi@qumranet.com>
Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

---
 arch/x86/kvm/mmu.c |    3 +++
 1 file changed, 3 insertions(+)

--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -1968,6 +1968,8 @@ static int mmu_shrink(int nr_to_scan, gf
 	list_for_each_entry(kvm, &vm_list, vm_list) {
 		int npages;
 
+		if (!down_read_trylock(&kvm->slots_lock))
+			continue;
 		spin_lock(&kvm->mmu_lock);
 		npages = kvm->arch.n_alloc_mmu_pages -
 			 kvm->arch.n_free_mmu_pages;
@@ -1980,6 +1982,7 @@ static int mmu_shrink(int nr_to_scan, gf
 		nr_to_scan--;
 
 		spin_unlock(&kvm->mmu_lock);
+		up_read(&kvm->slots_lock);
 	}
 	if (kvm_freed)
 		list_move_tail(&kvm_freed->vm_list, &vm_list);
